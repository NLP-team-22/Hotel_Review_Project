{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import Embedding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/unprocessed/tripadvisor_hotel_reviews.csv')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32])\n",
      "Review: tensor([  101, 12476,  2155, 10885,  2074,  2288,  2155,  1018,  4268,  1019,\n",
      "         1020,  2307,  2051,  1010,  4766, 19184,  1012,  4734,  7564,  3528,\n",
      "         1010,  3733,  6942,  2342,  1010,  1050,  1005,  1056,  4895, 23947,\n",
      "         2305,  2298,  7001,  2215,  1010,  2155,  2282,  2542,  2282,  1059,\n",
      "         1013,  1016,  2420,  9705,  4606,  5010,  1016,  7695,  1016, 28942,\n",
      "         1010, 11673,  2694,  7163,  1011, 16716,  1010,  3822,  2723,  4734,\n",
      "        24616, 21098,  1010,  3253, 10417,  2098,  2542, 29020,  1012,  2833,\n",
      "         3492,  2204,  1010,  6142,  2489,  3059,  1022,  9397,  1013,  1018,\n",
      "         3211,  5104,  2307,  1010,  2657, 23621,  2423,  2204,  1010, 13675,\n",
      "        13699,  2229,  3256,  6949, 24608,  2379,  3509,  1010,  3347,   102])\n",
      "Attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "Target: tensor(5.)\n",
      "641\n"
     ]
    }
   ],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "\tdef __init__(self, reviews, targets, tokenizer, max_length=512):\n",
    "\t\tself.reviews = reviews\n",
    "\t\tself.targets = targets\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_length = max_length\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.reviews)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treview = str(self.reviews[idx])\n",
    "\t\ttarget = self.targets[idx]\n",
    "\t\t\n",
    "\t\tencoding = self.tokenizer.encode_plus(\n",
    "\t\t\treview,\n",
    "\t\t\tadd_special_tokens=True,\n",
    "\t\t\tmax_length=self.max_length,\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tpadding='max_length',\n",
    "\t\t\treturn_attention_mask=True,\n",
    "\t\t\treturn_token_type_ids=False,\n",
    "\t\t\treturn_tensors='pt'\n",
    "\t\t)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"review\": encoding['input_ids'].squeeze(0),\n",
    "\t\t\t\"attention_mask\": encoding['attention_mask'].squeeze(0),\n",
    "\t\t\t\"target\": torch.tensor(target, dtype=torch.float)\n",
    "\t\t}\n",
    "\n",
    "max_length = 100\n",
    "dataset = ReviewDataset(df['Review'], df['Rating'], tokenizer, max_length=max_length)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_counts = df['Rating'].value_counts().sort_index()\n",
    "num_samples = len(df)\n",
    "class_weights = [num_samples/class_counts[i] for i in range(1, 6)]\n",
    "\n",
    "# Assign a weight to every sample in the dataset\n",
    "sample_weights = [class_weights[target - 1] for target in df['Rating']]\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights))\n",
    "\n",
    "# Create the DataLoader with the WeightedRandomSampler\n",
    "dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "for data in dataloader:\n",
    "\tprint(data['review'].shape)\n",
    "\tprint(data['attention_mask'].shape)\n",
    "\tprint(data['target'].shape)\n",
    "\tprint(\"Review:\", data['review'][0])\n",
    "\tprint(\"Attention mask:\", data['attention_mask'][0])\n",
    "\tprint(\"Target:\", data['target'][0])\n",
    "\tbreak\n",
    "\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Transformer Architecture </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "\t\tsuper(TransformerBlock, self).__init__()\n",
    "\t\tself.att = nn.MultiheadAttention(num_heads=num_heads, embed_dim=embed_dim)\n",
    "\t\tself.ffn = nn.Sequential(\n",
    "\t\t\tnn.Linear(embed_dim, ff_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(ff_dim, embed_dim),\n",
    "\t\t)\n",
    "\t\tself.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\t\tself.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\t\tself.dropout = nn.Dropout(rate)\n",
    "\n",
    "\tdef forward(self, x, mask=None):\n",
    "\t\t# Apply multi-head attention with optional mask\n",
    "\t\tattn_output, _ = self.att(x, x, x)\n",
    "\n",
    "\t\t# Apply dropout to attention output\n",
    "\t\tattn_output = self.dropout(attn_output)\n",
    "\n",
    "\t\t# Add & Normalize\n",
    "\t\tout1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "\t\t# Feed-forward network\n",
    "\t\tffn_output = self.ffn(out1)\n",
    "\n",
    "\t\t# Add & Normalize\n",
    "\t\tout2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\t\t# Apply dropout to entire output\n",
    "\t\treturn self.dropout(out2)\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_dim, max_length):\n",
    "\t\tsuper(TokenAndPositionEmbedding, self).__init__()\n",
    "\t\tself.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "\t\tself.pos_emb = nn.Embedding(max_length, embed_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size, seq_length = x.size()\n",
    "\t\tpositions = torch.arange(0, seq_length, dtype=torch.long, device=x.device).expand(batch_size, seq_length)\n",
    "\t\tx = self.token_emb(x)\n",
    "\t\tassert not torch.isnan(x).any(), \"There are NaN values in the token embeddings tensor.\"\n",
    "\t\tpos = self.pos_emb(positions)\n",
    "\t\treturn x + pos\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\tdef __init__(self, num_classes, embed_size, num_layers, heads, forward_expansion, dropout, max_length, vocab_size):\n",
    "\t\tsuper(Transformer, self).__init__()\n",
    "\n",
    "\t\tself.embedding = TokenAndPositionEmbedding(vocab_size, embed_size, max_length)\n",
    "\t\tself.layers = nn.ModuleList([TransformerBlock(embed_size, heads, forward_expansion, dropout)\n",
    "\t\t\t\t\t\t\t\t\t\tfor _ in range(num_layers)])\n",
    "\t\tself.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "\tdef forward(self, x, mask=None):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tassert not torch.isnan(x).any(), \"There are NaN values in the embedding layer output.\"\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\tassert not torch.isnan(x).any(), \"There are NaN values in the layer masking.\"\n",
    "\t\tx = torch.mean(x, dim=1)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 14/641 [00:05<04:28,  2.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Evaluation Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, average_loss)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     evaluate(dataloader, model, criterion)\n",
      "Cell \u001b[1;32mIn[132], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(reviews, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\t\n",
      "File \u001b[1;32mc:\\Users\\Ryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(num_classes=5, embed_size=32, num_layers=4, heads=2, forward_expansion=4, \n",
    "                        dropout=0.1, max_length=max_length, vocab_size=tokenizer.vocab_size)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer):\n",
    "\tmodel.train()\n",
    "\ttotal_loss = 0\n",
    "\tall_predictions = []\n",
    "\tall_targets = []\n",
    "\tpbar = tqdm.tqdm(dataloader)\n",
    "\tfor batch_idx, batch in enumerate(pbar):\n",
    "\t\t#print(batch['review'].shape)\n",
    "\t\t#print(batch['attention_mask'].shape)\n",
    "\t\t#print(batch['target'].shape)\n",
    "\t\t#print(batch['target'])\n",
    "\t\treviews = batch['review'].to(device)\n",
    "\t\ttargets = batch['target'].long().to(device)\n",
    "\t\ttargets = targets - 1\n",
    "\t\tmask = batch['attention_mask'].bool().to(device)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = model(reviews, mask=mask)\n",
    "\t\tloss = criterion(outputs, targets)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\ttotal_loss += loss.item()\t\n",
    "\n",
    "\t\t_, predicted = torch.max(outputs, 1)\n",
    "\t\tall_predictions.extend(predicted.cpu().numpy())\n",
    "\t\tall_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\taverage_loss = total_loss / len(dataloader)\n",
    "\taccuracy = accuracy_score(all_targets, all_predictions)\n",
    "\tprecision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "\tprint(\"Accuracy: \", accuracy)\n",
    "\tprint(\"Precision: \", precision)\n",
    "\tprint(\"Recall: \", recall)\n",
    "\tprint(\"F1 Score: \", f1)\n",
    "\tprint(\"Average Training Loss: \", average_loss)\n",
    "\n",
    "def evaluate(dataloader, model, criterion):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\treviews = batch['review'].to(device)\n",
    "\t\t\ttargets = batch['target'].long().to(device)\n",
    "\t\t\ttargets = targets - 1\n",
    "\t\t\tmask = batch['attention_mask'].bool().to(device)\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(reviews, mask=mask)\n",
    "\t\t\tloss = criterion(outputs, targets)\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\taverage_loss = total_loss / len(dataloader)\n",
    "\tprint(\"Average Evaluation Loss: \", average_loss)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(dataloader, model, criterion, optimizer)\n",
    "    evaluate(dataloader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluate model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Review': [\n",
    "        'It was fine.',\n",
    "        'Poor service and the food quality was below average.',\n",
    "        'An excellent stay, the staff was friendly and the location perfect.',\n",
    "        'The hotel is overpriced for the quality of the amenities provided.',\n",
    "        'Great experience, the room was well-appointed and the service impeccable.',\n",
    "\t\t'Good service',\n",
    "\t\t'Bad service'\n",
    "    ],\n",
    "    'Rating': [3, 2, 5, 2, 5, 4, 1]\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame(data)\n",
    "\n",
    "dataset = ReviewDataset(test_df['Review'], test_df['Rating'], tokenizer, max_length=max_length)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor idx in range(len(dataset)):\n",
    "\t\tsample = dataset[idx]\n",
    "\t\treview = sample['review'].unsqueeze(0).to(device)\n",
    "\t\tmask = sample['attention_mask'].unsqueeze(0).bool().to(device)\n",
    "\t\ttarget = sample['target'].unsqueeze(0).long().to(device)\n",
    "\t\ttarget = target - 1\n",
    "\n",
    "\t\toutput = model(review, mask=mask)\n",
    "\t\tprobabilities = torch.softmax(output, dim=1)\n",
    "\t\tpredictions = torch.argmax(probabilities, dim=1)\n",
    "\t\tprint(f\"Review: {test_df['Review'][idx]}\")\n",
    "\t\tprint(f\"Actual Rating: {target[0] + 1}\")\n",
    "\t\tprint(f\"Predicted Rating: {predictions[0] + 1}\")\n",
    "\t\tprint(f\"Probabilities: {probabilities}\")\n",
    "\t\tprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
